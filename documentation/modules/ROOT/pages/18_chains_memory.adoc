= Chains and Memory

:project-ai-name: quarkus-langchain-app

So far we explored how to use prompts with LLMs, however to really leverage the power of LLMs it is essential that you 
can build a conversation by referring to previous questions and answers and manage concurrent interactions. 

In this section we'll cover how we can acchieve this with the LangChain4j extension in Quarkus.

== Create an AI service with memory

Let's create an interface for our AI service, but with memory feature this time.

Create a new `AssistantWithMemory` Java interface in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import dev.langchain4j.service.MemoryId;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;

@RegisterAiService(/*chatMemoryProviderSupplier = RegisterAiService.BeanChatMemoryProviderSupplier.class*/)
public interface AssistantWithMemory {

    String chat(@MemoryId Integer id, @UserMessage String msg);

}
----

== Implement the ChatMemoryProvider

LangChain4j provides the interface `ChatMemoryProvider` to help us manage the memory of our conversations with the LLM.

Create a new `ChatMemoryBean` Java class in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import jakarta.annotation.PreDestroy;
import jakarta.enterprise.context.ApplicationScoped;

import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.ChatMemoryProvider;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;

@ApplicationScoped
public class ChatMemoryBean implements ChatMemoryProvider {

    private final Map<Object, ChatMemory> memories = new ConcurrentHashMap<>();

    @Override
    public ChatMemory get(Object memoryId) {
        return memories.computeIfAbsent(memoryId, id -> MessageWindowChatMemory.builder() #<1>
                .maxMessages(20) #<2>
                .id(memoryId)
                .build());
    }

    @PreDestroy
    public void close() {
        memories.clear();
    }
}
----
<1> If no chat memory exists yet, create a new instance
<2> Retain a maximum of 20 messages


== Create a Developer resource

Now let's create a resource to help us write some code.

Create a new `DeveloperResource` Java class in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import static dev.langchain4j.data.message.UserMessage.userMessage;
import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO;

import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Produces;
import jakarta.ws.rs.core.MediaType;

import dev.langchain4j.chain.ConversationalChain;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.TokenWindowChatMemory;
import dev.langchain4j.model.Tokenizer;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiTokenizer;
import dev.langchain4j.model.output.Response;

@Path("/code")
public class DeveloperResource {

    @Inject
    private ChatLanguageModel model;

    @GET
    @Path("/rest")
    @Produces(MediaType.TEXT_PLAIN)
    public void createRestEndpoint() {

        Tokenizer tokenizer = new OpenAiTokenizer();
        ChatMemory chatMemory = TokenWindowChatMemory.withMaxTokens(1000, tokenizer);

        UserMessage userMessage1 = userMessage(
                "How do I write a REST endpoint in Java using Quarkus? ");
        chatMemory.add(userMessage1);

        System.out.println("[User]: " + userMessage1.contents() + System.lineSeparator());

        final Response<AiMessage> response1 = model.generate(chatMemory.messages());
        chatMemory.add(response1.content());

        System.out.println("[LLM]: " + response1.content().text() + System.lineSeparator());

        UserMessage userMessage2 = userMessage(
                "Create a test of the first point. " +
                        "Be short, 15 lines of code maximum.");
        chatMemory.add(userMessage2);

        System.out.println("[User]: " + userMessage2.contents() + System.lineSeparator());

        final Response<AiMessage> response2 = model.generate(chatMemory.messages());

        System.out.println("[LLM]: " + response2.content().text() + System.lineSeparator());

    }
}
----

== Invoke the endpoint

You can check your prompt implementation by pointing your browser to http://localhost:8080/code/rest[window=_blank]

You can also run the following command in your terminal:

[.console-input]
[source,bash]
----
curl localhost:8080/code/rest
----

The result will be in the logs of your Quarkus application (ie. the terminal where you're running the `quarkus dev` command). An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
[User]: How do I write a REST endpoint in Java using Quarkus? 

[LLM]: To create a REST endpoint in Java using Quarkus, you can follow these steps:

1. Create a new Quarkus project using the Quarkus Maven plugin or Quarkus CLI.
2. Create a new Java class for your REST endpoint. You can annotate this class with `@Path` to define the base URL path for your endpoint.
3. Add methods to your class and annotate them with `@GET`, `@POST`, `@PUT`, or `@DELETE` annotations to define the HTTP method for each endpoint.
4. Use the `@Produces` and `@Consumes` annotations to specify the content type of the responses and requests.
5. Use the `@PathParam` and `@QueryParam` annotations to capture path and query parameters in your endpoint methods.
6. Implement the logic for your endpoint methods.
7. Build and run your Quarkus project to start the application and test your REST endpoint.

Here's an example of a simple REST endpoint class in Quarkus:

```java
import javax.ws.rs.*;
import javax.ws.rs.core.MediaType;

@Path("/hello")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class HelloResource {

    @GET
    public String sayHello() {
        return "Hello, World!";
    }

    @GET
    @Path("/{name}")
    public String sayHelloTo(@PathParam("name") String name) {
        return "Hello, " + name + "!";
    }
}
```

This class defines two REST endpoints: `/hello` for saying hello to the world, and `/hello/{name}` for saying hello to a specific name. You can access these endpoints at `http://localhost:8080/hello` and `http://localhost:8080/hello/{name}` respectively.


[User]: Create a test of the first point. Be short, 15 lines of code maximum.

[LLM]: Here's an example of a simple test for the `sayHello` endpoint in Quarkus using JUnit:

```java
import io.quarkus.test.junit.QuarkusTest;
import io.restassured.RestAssured;
import org.junit.jupiter.api.Test;

import static io.restassured.RestAssured.given;
import static org.hamcrest.CoreMatchers.is;

@QuarkusTest
public class HelloResourceTest {

    @Test
    public void testSayHelloEndpoint() {
        given()
          .when().get("/hello")
          .then()
             .statusCode(200)
             .body(is("Hello, World!"));
    }
}
```

In this test, we are using the QuarkusTest annotation to run the test in the Quarkus test environment. The `testSayHelloEndpoint` method sends a GET request to the `/hello` endpoint and verifies that the response status code is 200 and that the response body is "Hello, World!".
```

----

Let's now get some help to learn a little bit about Kubernetes. 

Add a new `generateKubernetes()` method to the `DeveloperResource` class:

[.console-input]
[source,java]
----
    @GET
    @Path("/k8s")
    @Produces(MediaType.TEXT_PLAIN)
    public void generateKubernetes() {

        ConversationalChain chain = ConversationalChain.builder()
                .chatLanguageModel(model)
                .build();

        String userMessage1 = "Can you give a brief explanation of Kubernetes, 3 lines max?";
        System.out.println("[User]: " + userMessage1 + System.lineSeparator());

        String answer1 = chain.execute(userMessage1);
        System.out.println("[LLM]: " + answer1 + System.lineSeparator());

        String userMessage2 = "Can you give me a YAML example to deploy an application for that?";
        System.out.println("[User]: " + userMessage2 + System.lineSeparator());

        String answer2 = chain.execute(userMessage2);
        System.out.println("[LLM]: " + answer2);

    }
----

== Invoke the endpoint

You can check your prompt implementation by pointing your browser to http://localhost:8080/code/k8s[window=_blank]

You can also run the following command:

[.console-input]
[source,bash]
----
curl localhost:8080/code/k8s
----

The result will be onc again in your Quarkus application logs. An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
[User]: Can you give a brief explanation of Kubernetes, 3 lines max?

[LLM]: Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It simplifies the process of managing and coordinating large numbers of containers across multiple clusters. Kubernetes provides a scalable and efficient way to deploy and manage containerized applications in a production-ready environment.


[User]: Can you give me a YAML example to deploy an application for that?

[LLM]: Sure! Here is an example of a simple YAML file that deploys a sample application using Kubernetes:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: sample-app
        image: nginx:latest
        ports:
        - containerPort: 80
```

Save this YAML file as `sample-app-deployment.yaml` and apply it using the `kubectl apply -f sample-app-deployment.yaml` command to deploy the sample application with 3 replicas running NGINX.
----

== How to index a conversation

We can use the LangChain4j extension to index a conversation so we can reuse it.

Let's inject an instance of the AssistenWithMemory class and add a new `guessWho()` method to our `DeveloperResource`:

[.console-input]
[source,java]
----
    @Inject
    AssistantWithMemory assistant;

    @GET
    @Path("/guess")
    @Produces(MediaType.TEXT_PLAIN)
    public void guessWho() {

        System.out.println(assistant.chat(1, "Hello, my name is Klaus, and I'm a Doctor"));

        System.out.println(assistant.chat(2, "Hello, my name is Francine, and I'm a Lawyer"));

        System.out.println(assistant.chat(1, "What is my name?"));

        System.out.println(assistant.chat(2, "What is my profession?"));

    }

----

== Invoke the endpoint

You can check your implementation by pointing your browser to http://localhost:8080/code/guess[window=_blank]

You can also run the following command:

[.console-input]
[source,bash]
----
curl localhost:8080/code/guess
----

The result will be at your Quarkus terminal. An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
Hello Klaus, it's nice to meet you. What type of doctor are you?
Hello Francine, nice to meet you! How can I assist you today?
Your name is Klaus.
Your profession is a Lawyer. You are legally trained and licensed to represent clients in legal matters.
----

NOTE: You might be confused by the responses (ie. Klaus is not a lawyer but a doctor). Take a close look at the IDs of our calls to the assistant. Do you notice that the last question was in fact directed to Francine with ID=2? We were indeed able to maintain 2 separate and concurrent conversations with the LLM!
