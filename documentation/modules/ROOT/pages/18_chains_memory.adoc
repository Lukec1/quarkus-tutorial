= Chains and Memory

:project-ai-name: quarkus-langchain-app

If you only stick with prompts, you rely solely on the current message.

But LangChain4j can help you do more, which this section covers.

It shows you how to create conversations with the LLM, refer to past answers, and manage concurrent interactions.



IMPORTANT: We keep assuming that you'll be working inside the project folder that was created before. In this case, `{project-ai-name}`.

== Create the AI service with memory

Let's create an interface for our AI service, but with memory feature this time.

Create a new `AssistantWithMemory` Java interface in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import dev.langchain4j.service.MemoryId;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;

@RegisterAiService(/*chatMemoryProviderSupplier = RegisterAiService.BeanChatMemoryProviderSupplier.class*/)
public interface AssistantWithMemory {

    String chat(@MemoryId Integer id, @UserMessage String msg);

}
----

== Implement the ChatMemoryProvider

LangChain4j provides the interface `ChatMemoryProvider` to help us manage the memory of our conversations with the LLM.

Create a new `ChatMemoryBean` Java class in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import jakarta.annotation.PreDestroy;
import jakarta.enterprise.context.ApplicationScoped;

import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.ChatMemoryProvider;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;

@ApplicationScoped
public class ChatMemoryBean implements ChatMemoryProvider {

    private final Map<Object, ChatMemory> memories = new ConcurrentHashMap<>();

    @Override
    public ChatMemory get(Object memoryId) {
        return memories.computeIfAbsent(memoryId, id -> MessageWindowChatMemory.builder()
                .maxMessages(20)
                .id(memoryId)
                .build());
    }

    @PreDestroy
    public void close() {
        memories.clear();
    }
}
----

== Create a Developer resource

Now we create a resource to help us to create code.

Create a new `DeveloperResource` Java class in `src/main/java` in the `com.redhat.developers` package with the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import static dev.langchain4j.data.message.UserMessage.userMessage;
import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO;

import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Produces;
import jakarta.ws.rs.core.MediaType;

import dev.langchain4j.chain.ConversationalChain;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.TokenWindowChatMemory;
import dev.langchain4j.model.Tokenizer;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiTokenizer;
import dev.langchain4j.model.output.Response;

@Path("/code")
public class DeveloperResource {

    @Inject
    private ChatLanguageModel model;

    @GET
    @Path("/rest")
    @Produces(MediaType.TEXT_PLAIN)
    public void createRestEndpoint() {

        Tokenizer tokenizer = new OpenAiTokenizer(GPT_3_5_TURBO);
        ChatMemory chatMemory = TokenWindowChatMemory.withMaxTokens(1000, tokenizer);

        UserMessage userMessage1 = userMessage(
                "How to write a REST endpoint in Java using Quarkus? ");
        chatMemory.add(userMessage1);

        System.out.println("[User]: " + userMessage1.text() + System.lineSeparator());

        final Response<AiMessage> response1 = model.generate(chatMemory.messages());
        chatMemory.add(response1.content());

        System.out.println("[LLM]: " + response1.content().text() + System.lineSeparator());

        UserMessage userMessage2 = userMessage(
                "Create a test of the first point? " +
                        "Be short, 15 lines of code maximum.");
        chatMemory.add(userMessage2);

        System.out.println("[User]: " + userMessage2.text() + System.lineSeparator());

        final Response<AiMessage> response2 = model.generate(chatMemory.messages());

        System.out.println("[LLM]: " + response2.content().text() + System.lineSeparator());

    }
}
----

== Invoke the endpoint

You can check your prompt implementation by pointing your browser to http://localhost:8080/code/rest[window=_blank]

You can also run the following command:

[.console-input]
[source,bash]
----
curl localhost:8080/code/rest
----

The result will be at your Quarkus terminal. An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
[User]: How to write a REST endpoint in Java using Quarkus?

[LLM]: To write a REST endpoint in Java using Quarkus, you can follow these steps:

1. Create a new Quarkus project using the Quarkus Maven Plugin or Quarkus CLI. You can refer to the Quarkus documentation for detailed instructions.

2. Create a new Java class for your REST endpoint. This class should be annotated with `@Path` to specify the base path for the endpoint and `@GET`, `@POST`, `@PUT`, `@DELETE`, etc., annotations to define the HTTP methods supported by the endpoint.

```java
import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;

@Path("/hello")
public class HelloResource {

    @GET
    @Produces(MediaType.TEXT_PLAIN)
    public String hello() {
        return "Hello, World!";
    }
}
```

3. Make sure to add the necessary dependencies in your `pom.xml` file to enable REST services and JSON support in your Quarkus project.

```xml
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-resteasy</artifactId>
</dependency>
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-resteasy-jsonb</artifactId>
</dependency>
```

4. Build and run your Quarkus application using the command line or your favorite IDE.

5. Access your REST endpoint by making an HTTP request to the appropriate URL. For example, if your endpoint is defined with a base path of `/hello`, you can access it at `http://localhost:8080/hello`.

That's it! You have now successfully created a REST endpoint in Java using Quarkus. You can further customize your endpoint by adding request parameters, path parameters, response headers, error handling, and other features supported by JAX-RS and Quarkus.


[User]: Create a test of the first point? Be short, 15 lines of code maximum.

[LLM]: You can create a test for the `HelloResource` endpoint using Quarkus Test Framework in just a few lines of code. Here's an example of a test class for the `HelloResource` endpoint:

``` java
import io.quarkus.test.junit.QuarkusTest;
import io.restassured.RestAssured;
import org.junit.jupiter.api.Test;

import static io.restassured.RestAssured.given;
import static org.hamcrest.CoreMatchers.is;

@QuarkusTest
public class HelloResourceTest {

    @Test
    public void testHelloEndpoint() {
        given()
          .when().get("/hello")
          .then()
             .statusCode(200)
             .body(is("Hello, World!"));
    }
}
```

In this test class, we are using RestAssured to make an HTTP GET request to the `/hello` endpoint and verifying that the response status code is 200 and the response body is "Hello, World!". The `@QuarkusTest` annotation is used to start a test Quarkus application before running the tests.

----

Let's now get some help to learn a little bit about Kubernetes. 

Here is an updated version of our `DeveloperResource` class:

[.console-input]
[source,java]
----
package com.redhat.developers;

import static dev.langchain4j.data.message.UserMessage.userMessage;
import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO;

import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Produces;
import jakarta.ws.rs.core.MediaType;

import dev.langchain4j.chain.ConversationalChain;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.TokenWindowChatMemory;
import dev.langchain4j.model.Tokenizer;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiTokenizer;
import dev.langchain4j.model.output.Response;

@Path("/code")
public class DeveloperResource {

    @Inject
    private ChatLanguageModel model;

    @GET
    @Path("/rest")
    @Produces(MediaType.TEXT_PLAIN)
    public void createRestEndpoint() {

        Tokenizer tokenizer = new OpenAiTokenizer(GPT_3_5_TURBO);
        ChatMemory chatMemory = TokenWindowChatMemory.withMaxTokens(1000, tokenizer);

        UserMessage userMessage1 = userMessage(
                "How to write a REST endpoint in Java? ");
        chatMemory.add(userMessage1);

        System.out.println("[User]: " + userMessage1.text() + System.lineSeparator());

        final Response<AiMessage> response1 = model.generate(chatMemory.messages());
        chatMemory.add(response1.content());

        System.out.println("[LLM]: " + response1.content().text() + System.lineSeparator());

        UserMessage userMessage2 = userMessage(
                "Create a test of the first point? " +
                        "Be short, 15 lines of code maximum.");
        chatMemory.add(userMessage2);

        System.out.println("[User]: " + userMessage2.text() + System.lineSeparator());

        final Response<AiMessage> response2 = model.generate(chatMemory.messages());

        System.out.println("[LLM]: " + response2.content().text() + System.lineSeparator());

    }

    @GET
    @Path("/k8s")
    @Produces(MediaType.TEXT_PLAIN)
    public void generateKubernetes() {

        ConversationalChain chain = ConversationalChain.builder()
                .chatLanguageModel(model)
                .build();

        String userMessage1 = "Can you give a brief explanation of Kubernetes, 3 lines max?";
        System.out.println("[User]: " + userMessage1 + System.lineSeparator());

        String answer1 = chain.execute(userMessage1);
        System.out.println("[LLM]: " + answer1 + System.lineSeparator());

        String userMessage2 = "Can you give me a YAML example to deploy an application for that?";
        System.out.println("[User]: " + userMessage2 + System.lineSeparator());

        String answer2 = chain.execute(userMessage2);
        System.out.println("[LLM]: " + answer2);

    }
}
----

== Invoke the endpoint

You can check your prompt implementation by pointing your browser to http://localhost:8080/code/k8s[window=_blank]

You can also run the following command:

[.console-input]
[source,bash]
----
curl localhost:8080/code/k8s
----

The result will be at your Quarkus terminal. An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
[User]: Can you give a brief explanation of Kubernetes, 3 lines max?

[LLM]: Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It simplifies the process of managing and coordinating large numbers of containers across multiple clusters. Kubernetes provides a scalable and efficient way to deploy and manage containerized applications in a production-ready environment.


[User]: Can you give me a YAML example to deploy an application for that?

[LLM]: Sure! Here is an example of a simple YAML file that deploys a sample application using Kubernetes:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: sample-app
        image: nginx:latest
        ports:
        - containerPort: 80
```

Save this YAML file as `sample-app-deployment.yaml` and apply it using the `kubectl apply -f sample-app-deployment.yaml` command to deploy the sample application with 3 replicas running NGINX.
----

== How to index a conversation

We can use LangChain4j to index a conversation so we can reuse it.

Let's update our `DeveloperResource` ith the following contents:

[.console-input]
[source,java]
----
package com.redhat.developers;

import static dev.langchain4j.data.message.UserMessage.userMessage;
import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO;

import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Produces;
import jakarta.ws.rs.core.MediaType;

import dev.langchain4j.chain.ConversationalChain;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.TokenWindowChatMemory;
import dev.langchain4j.model.Tokenizer;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiTokenizer;
import dev.langchain4j.model.output.Response;

@Path("/code")
public class DeveloperResource {

    @Inject
    private ChatLanguageModel model;

    @Inject
    AssistantWithMemory assistant;

    @GET
    @Path("/rest")
    @Produces(MediaType.TEXT_PLAIN)
    public void createRestEndpoint() {

        Tokenizer tokenizer = new OpenAiTokenizer(GPT_3_5_TURBO);
        ChatMemory chatMemory = TokenWindowChatMemory.withMaxTokens(1000, tokenizer);

        UserMessage userMessage1 = userMessage(
                "How to write a REST endpoint in Java? ");
        chatMemory.add(userMessage1);

        System.out.println("[User]: " + userMessage1.text() + System.lineSeparator());

        final Response<AiMessage> response1 = model.generate(chatMemory.messages());
        chatMemory.add(response1.content());

        System.out.println("[LLM]: " + response1.content().text() + System.lineSeparator());

        UserMessage userMessage2 = userMessage(
                "Create a test of the first point? " +
                        "Be short, 15 lines of code maximum.");
        chatMemory.add(userMessage2);

        System.out.println("[User]: " + userMessage2.text() + System.lineSeparator());

        final Response<AiMessage> response2 = model.generate(chatMemory.messages());

        System.out.println("[LLM]: " + response2.content().text() + System.lineSeparator());

    }

    @GET
    @Path("/k8s")
    @Produces(MediaType.TEXT_PLAIN)
    public void generateKubernetes() {

        ConversationalChain chain = ConversationalChain.builder()
                .chatLanguageModel(model)
                .build();

        String userMessage1 = "Can you give a brief explanation of Kubernetes, 3 lines max?";
        System.out.println("[User]: " + userMessage1 + System.lineSeparator());

        String answer1 = chain.execute(userMessage1);
        System.out.println("[LLM]: " + answer1 + System.lineSeparator());

        String userMessage2 = "Can you give me a YAML example to deploy an application for that?";
        System.out.println("[User]: " + userMessage2 + System.lineSeparator());

        String answer2 = chain.execute(userMessage2);
        System.out.println("[LLM]: " + answer2);

    }

    @GET
    @Path("/guess")
    @Produces(MediaType.TEXT_PLAIN)
    public void guessWho() {

        System.out.println(assistant.chat(1, "Hello, my name is Klaus, and I'm a Doctor"));

        System.out.println(assistant.chat(2, "Hello, my name is Francine, and I'm a Lawyer"));

        System.out.println(assistant.chat(1, "What is my name?"));

        System.out.println(assistant.chat(2, "What is my profession?"));

    }
}
----

== Invoke the endpoint

You can check your implementation by pointing your browser to http://localhost:8080/code/guess[window=_blank]

You can also run the following command:

[.console-input]
[source,bash]
----
curl localhost:8080/code/guess
----

The result will be at your Quarkus terminal. An example of output (it can vary on each prompt execution):

[.console-output]
[source,text]
----
Hello Klaus, it's nice to meet you. What type of doctor are you?
Hello Francine, nice to meet you! How can I assist you today?
Your name is Klaus.
Your profession is a Lawyer. You are legally trained and licensed to represent clients in legal matters.
----
